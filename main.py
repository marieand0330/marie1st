"""
Main module for ETF Daily Briefing Scraper
"""
import asyncio
import logging
import sys
import os
from datetime import datetime

from flask import Flask, jsonify
from config import LOG_LEVEL, LOG_FORMAT, LOG_FILE, TICKERS, TEST_TICKERS
from scraper import ETFScraper
from telegram_sender import send_message, send_html_content, send_chart_analysis
from stock_data import get_stock_data

# Initialize Flask app
app = Flask(__name__)

def setup_logging():
    """Configure logging for the application"""
    level = getattr(logging, LOG_LEVEL.upper(), logging.INFO)
    logging.basicConfig(
        level=level,
        format=LOG_FORMAT,
        handlers=[
            logging.StreamHandler(stream=sys.stdout),
            logging.FileHandler(LOG_FILE)
        ]
    )
    logger = logging.getLogger(__name__)
    logger.info("Logging configured successfully")
    return logger

async def run_once(tickers=None, logger=None):
    """Run scraper once for testing"""
    if logger is None:
        logger = setup_logging()
    return await run_scraper(tickers, logger)

async def run_scraper(tickers=None, logger=None):
    """Run the scraper for specified tickers"""
    if logger is None:
        logger = setup_logging()

    # ì§€ì •ëœ ìˆœì„œë¡œ í‹°ì»¤ ì •ë ¬
    default_order = ["IGV", "SOXL", "BLK", "IVZ", "BRKU"]
    if tickers is None:
        tickers = default_order
    else:
        # ì…ë ¥ëœ í‹°ì»¤ë“¤ì„ ì§€ì •ëœ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        tickers = sorted(tickers, key=lambda x: default_order.index(x) if x in default_order else len(default_order))
    
    logger.info(f"Running scrape for tickers: {', '.join(tickers)}")

    scraper = None
    try:
        scraper = ETFScraper()
        results = await asyncio.wait_for(
            scraper.scrape_all_tickers(tickers),
            timeout=120
        )

        # Send to Telegram
        try:
            logger.info("í…”ë ˆê·¸ë¨ìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡ ì‹œì‘")
            today_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")

            header_message = f"ğŸ“Š <b>ETF ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ ({today_date})</b>\n\n"
            await send_message(header_message)

            for result in results:
                ticker = result.split(':')[0].strip()
                html_content = result.replace(f"{ticker}:", "")

                await send_html_content(ticker, html_content)

                try:
                    chart_data = get_stock_data(ticker)
                    if chart_data:
                        await send_chart_analysis(ticker, chart_data)
                except Exception as e:
                    logger.error(f"ì°¨íŠ¸ ë°ì´í„° ì „ì†¡ ì‹¤íŒ¨ ({ticker}): {e}")

                await asyncio.sleep(1)

            logger.info("í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False

    except asyncio.TimeoutError:
        logger.error("Overall scraping operation timed out")
        # Generate fallback results for all tickers
        results = []
        for ticker in tickers:
            fallback = f"{ticker}:\në°ì¼ë¦¬ ë¸Œë¦¬í•‘\n\nì‹œê°„ ì´ˆê³¼ë¡œ ì¸í•´ ë¸Œë¦¬í•‘ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ í™•ì¸í•´ì£¼ì„¸ìš”: https://invest.zum.com/{'etf' if ticker in ['IGV', 'SOXL', 'BRKU'] else 'stock'}/{ticker}/"
            results.append(fallback)

        # Send fallback message to Telegram
        try:
            logger.info("í…”ë ˆê·¸ë¨ìœ¼ë¡œ íƒ€ì„ì•„ì›ƒ ì•Œë¦¼ ì „ì†¡")
            today_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")

            error_message = (
                f"âš ï¸ <b>ETF ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ ì˜¤ë¥˜ ({today_date})</b>\n\n"
                f"ìŠ¤í¬ë˜í•‘ ì‘ì—… ì¤‘ íƒ€ì„ì•„ì›ƒì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. "
                f"ì¼ë¶€ ETF/ì£¼ì‹ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                f"ì˜í–¥ë°›ì€ í‹°ì»¤: {', '.join(tickers)}"
            )
            await send_message(error_message)

            for result in results:
                ticker = result.split(':')[0].strip()
                await send_message(result)

            logger.info("í…”ë ˆê·¸ë¨ íƒ€ì„ì•„ì›ƒ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"í…”ë ˆê·¸ë¨ íƒ€ì„ì•„ì›ƒ ì•Œë¦¼ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

    except Exception as e:
        logger.error(f"Error in scrape: {e}")
        return False
    finally:
        if scraper:
            scraper.close()

@app.route('/trigger-scrape', methods=['POST'])
async def trigger_scrape():
    """HTTP endpoint to trigger ETF scraping"""
    logger = setup_logging()
    success = await run_scraper(logger=logger)

    return jsonify({
        'success': success,
        'timestamp': datetime.now().isoformat(),
        'message': 'Scraping completed successfully' if success else 'Scraping failed'
    })

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({'status': 'healthy'})

if __name__ == "__main__":
    logger = setup_logging()
    logger.info("Starting ETF Daily Briefing Scraper")
    app.run(host='0.0.0.0', port=8000)
