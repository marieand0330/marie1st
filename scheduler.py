"""
Scheduler module for running ETF scraper at specified times
"""
import asyncio
import logging
import time
from datetime import datetime, timedelta

import schedule

from config import SCHEDULE_HOUR, SCHEDULE_MINUTE, TICKERS
from scraper import ETFScraper
from telegram_sender import send_message, send_html_content, send_chart_analysis
from stock_data import get_stock_data

logger = logging.getLogger(__name__)

class ETFScraperScheduler:
    """
    Scheduler for running ETF scraper on a daily schedule
    """
    def __init__(self, tickers=None):
        """
        Initialize the scheduler
        
        Args:
            tickers (list, optional): List of tickers to scrape. Defaults to config.TICKERS.
        """
        self.tickers = tickers or TICKERS
        self.scraper = None
        
    async def run_scraper(self):
        """
        Run the scraper for all configured tickers
        """
        logger.info(f"Starting scheduled scraping task at {datetime.now()}")
        
        try:
            self.scraper = ETFScraper()
            
            try:
                # Use a timeout for the entire scraping operation (2 minutes)
                results = await asyncio.wait_for(
                    self.scraper.scrape_all_tickers(self.tickers),
                    timeout=120
                )
                
                # Print all results
                print("\n" + "="*50)
                print(f"ETF DAILY BRIEFINGS - {datetime.now().strftime('%Y-%m-%d')}")
                print("="*50)
                for result in results:
                    print(f"\n{result}")
                    print("-"*50)
                    
                logger.info(f"Completed scraping task for {len(self.tickers)} tickers")
                
                # í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ì†¡
                try:
                    logger.info("í…”ë ˆê·¸ë¨ìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡ ì‹œì‘")
                    today_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
                    
                    # í—¤ë” ë©”ì‹œì§€ ì „ì†¡
                    header_message = f"ğŸ“Š <b>ETF ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ ({today_date})</b>\n\n"
                    await send_message(header_message)
                    
                    # ê° ETF/ì£¼ì‹ ë¸Œë¦¬í•‘ ì „ì†¡
                    for result in results:
                        # í‹°ì»¤ ì¶”ì¶œ (ê²°ê³¼ì˜ ì²« ë¶€ë¶„ì€ í•­ìƒ "TICKER:"ë¡œ ì‹œì‘)
                        ticker = result.split(':')[0].strip()
                        
                        # HTML ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë˜í•‘ëœ ê²°ê³¼)
                        html_content = result.replace(f"{ticker}:", "")
                        await send_html_content(ticker, html_content)
                        
                        # ì°¨íŠ¸ ë¶„ì„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë° ì „ì†¡
                        try:
                            chart_data = get_stock_data(ticker)
                            if chart_data:
                                await send_chart_analysis(ticker, chart_data)
                        except Exception as e:
                            logger.error(f"ì°¨íŠ¸ ë°ì´í„° ì „ì†¡ ì‹¤íŒ¨ ({ticker}): {e}")
                        
                        # ê° í‹°ì»¤ ì‚¬ì´ì— ì•½ê°„ì˜ ì‹œê°„ ê°„ê²© ì¶”ê°€
                        await asyncio.sleep(1)
                        
                    logger.info("í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ")
                    
                except Exception as e:
                    logger.error(f"í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                
            except asyncio.TimeoutError:
                logger.error("Overall scraping operation timed out in scheduled run")
                # Generate fallback results for all tickers
                results = []
                for ticker in self.tickers:
                    is_etf = ticker in ['IGV', 'SOXL', 'BRKU']
                    ticker_type = 'etf' if is_etf else 'stock'
                    fallback = f"{ticker}:\në°ì¼ë¦¬ ë¸Œë¦¬í•‘\n\nì‹œê°„ ì´ˆê³¼ë¡œ ì¸í•´ ë¸Œë¦¬í•‘ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ í™•ì¸í•´ì£¼ì„¸ìš”: https://invest.zum.com/{ticker_type}/{ticker}/"
                    results.append(fallback)
                    
                # Print fallback results
                print("\n" + "="*50)
                print(f"ETF DAILY BRIEFINGS - FALLBACK RESULTS - {datetime.now().strftime('%Y-%m-%d')}")
                print("="*50)
                for result in results:
                    print(f"\n{result}")
                    print("-"*50)
                
                logger.info("Printed fallback results due to timeout")
                
                # í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì˜¤ë¥˜ ì•Œë¦¼ ì „ì†¡
                try:
                    logger.info("í…”ë ˆê·¸ë¨ìœ¼ë¡œ íƒ€ì„ì•„ì›ƒ ì•Œë¦¼ ì „ì†¡")
                    today_date = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
                    
                    # ì˜¤ë¥˜ ë©”ì‹œì§€ ì „ì†¡
                    error_message = (
                        f"âš ï¸ <b>ETF ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ ì˜¤ë¥˜ ({today_date})</b>\n\n"
                        f"ìŠ¤í¬ë˜í•‘ ì‘ì—… ì¤‘ íƒ€ì„ì•„ì›ƒì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. "
                        f"ì¼ë¶€ ETF/ì£¼ì‹ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                        f"ì˜í–¥ë°›ì€ í‹°ì»¤: {', '.join(self.tickers)}"
                    )
                    await send_message(error_message)
                    
                    # ê° í‹°ì»¤ì— ëŒ€í•œ ëŒ€ì²´ ë©”ì‹œì§€ ì „ì†¡
                    for result in results:
                        ticker = result.split(':')[0].strip()
                        await send_message(result)
                        
                    logger.info("í…”ë ˆê·¸ë¨ íƒ€ì„ì•„ì›ƒ ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ")
                    
                except Exception as e:
                    logger.error(f"í…”ë ˆê·¸ë¨ íƒ€ì„ì•„ì›ƒ ì•Œë¦¼ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
        except Exception as e:
            logger.error(f"Error running scheduled task: {e}")
        finally:
            if self.scraper:
                self.scraper.close()
                self.scraper = None
    
    def schedule_daily_run(self):
        """
        Schedule daily execution at the configured time
        """
        logger.info(f"Scheduling daily run at {SCHEDULE_HOUR:02d}:{SCHEDULE_MINUTE:02d}")
        
        # Schedule the job to run daily at specified time
        schedule.every().day.at(f"{SCHEDULE_HOUR:02d}:{SCHEDULE_MINUTE:02d}").do(
            lambda: asyncio.run(self.run_scraper())
        )
        
        # Also run immediately for the first time
        logger.info("Running initial scraping job")
        asyncio.run(self.run_scraper())
        
    def run_continuously(self):
        """
        Run the scheduler continuously
        """
        logger.info("Starting scheduler")
        
        try:
            while True:
                schedule.run_pending()
                
                # Calculate time until next scheduled run
                next_run = schedule.next_run()
                if next_run:
                    now = datetime.now()
                    time_until_next = next_run - now
                    hours, remainder = divmod(time_until_next.seconds, 3600)
                    minutes, seconds = divmod(remainder, 60)
                    logger.info(f"Next scheduled run at {next_run.strftime('%Y-%m-%d %H:%M:%S')} "
                               f"(in {hours}h {minutes}m {seconds}s)")
                
                # Sleep for a minute before checking again
                time.sleep(60)
                
        except KeyboardInterrupt:
            logger.info("Scheduler stopped by user")
        except Exception as e:
            logger.error(f"Scheduler error: {e}")
